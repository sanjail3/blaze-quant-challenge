"""
 Submission by - Shrirang Savale (Team Leader) and Uday Joshi
 Institute - IIIT Nagpur
 Team - CompetitionSmashers

 All 3 models(Linear regression with time series, LSTM, GRU, LSTM and their variants) - https://github.com/Whitehatdown/Ethereum-Prediction-Model
 Training,Testing and Validation dataset for each model has been included. One dataset includes data for last 5 years, one has data for last 3 year and one has data for last 1 year. Data for 5 year is raw while data for 3 year has been cleaned to given better results... 
 
 
 """

"""
This is just one of our sample models we used - For more details visit GITHUB repository link given at the top of this file

##############################################################################################################
# Use the below code to train the model - Its a LSTM model. So training takes 10-15mins if we train for 2000 epoches...
##############################################################################################################

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from keras.layers import LSTM, Dense, Dropout
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.dates as mdates
from sklearn.preprocessing import MinMaxScaler
from sklearn import linear_model
from keras.models import Sequential
from keras.layers import Dense
import keras.backend as K
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam
from keras.models import load_model
from keras.layers import LSTM
from keras.utils import plot_model

from tensorflow.python.client import device_lib
from keras import backend as K

print(device_lib.list_local_devices())
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

if len(tf.config.experimental.list_physical_devices('GPU')) > 0:
    print("Using GPU")
    # Set TensorFlow to use GPU memory growth
    for device in device_lib.list_local_devices():
        if device.device_type == 'GPU':
            tf.config.experimental.set_memory_growth(device, True)
    K.set_floatx('float32')
    K.set_epsilon(1e-7)

df = pd.read_csv('modified_file_with_technical_parameters_new.csv', na_values=['null'], index_col='Date', parse_dates=True, infer_datetime_format=True)
df.head()

print("Dataframe Shape: ", df.shape)
print("Null Value Present: ", df.isnull().values.any())

features = ['Open', 'High', 'Low', 'Volume', 'Adj Close', 'Yesterday Price', '30 Day Average', '7 Day Average',
            '10-day SMA', '20-day SMA', '50-day SMA', '12-day EMA', '26-day EMA', '50-day EMA']

df['Tomorrow Price'] = df['Close'].shift(-1)
df = df.dropna()

scaler = MinMaxScaler()
feature_transform = scaler.fit_transform(df[features])
feature_transform = pd.DataFrame(columns=features, data=feature_transform, index=df.index)

timesplit = TimeSeriesSplit(n_splits=5)
for train_index, test_index in timesplit.split(feature_transform):
    X_train, X_test = feature_transform[:len(train_index)], feature_transform[len(train_index):(len(train_index) + len(test_index))]
    y_train, y_test = df['Tomorrow Price'][:len(train_index)].values.ravel(), df['Tomorrow Price'][len(train_index):(len(train_index) + len(test_index))].values.ravel()

trainX = np.array(X_train)
testX = np.array(X_test)
X_train = trainX.reshape(X_train.shape[0], 1, X_train.shape[1])
X_test = testX.reshape(X_test.shape[0], 1, X_test.shape[1])

# Here our real journey begins...
lstm = Sequential()
lstm.add(LSTM(32, input_shape=(1, trainX.shape[1]), activation='relu', return_sequences=True))
lstm.add(LSTM(64, activation='relu', return_sequences=True))
lstm.add(LSTM(128, activation='relu', return_sequences=False))  # Last layer does not return sequences
lstm.add(Dense(1))

lstm.compile(loss='mean_squared_error', optimizer='adam')
plot_model(lstm, show_shapes=True, show_layer_names=True)

with tf.device('/GPU:0'):
    history = lstm.fit(X_train, y_train, epochs=5, batch_size=8, verbose=1, shuffle=False)

y_pred = lstm.predict(X_test)
plt.plot(y_test, label="True Tomorrow's Price")
plt.plot(y_pred, label="Predicted Tomorrow's Price")
plt.title("Prediction vs True Tomorrow's Price")
plt.xlabel('Time Scale')
plt.ylabel('Scaled USD')
plt.legend()
plt.show()

#lstm.save('lstm_model_Tomorrow_price_2_step.h5')

final_X_test = np.array(feature_transform.iloc[-1]).reshape(1, 1, len(features)+1)
final_tomorrow_price = lstm.predict(final_X_test)[0, 0]
final_date = feature_transform.index[-1] + pd.DateOffset(days=1)
print(f"Final Predicted Tomorrow's Price ({final_date}): {final_tomorrow_price}")

import joblib
#joblib.dump(scaler, 'feature_scaler.pkl')

#np.save('train_indices.npy', train_index)



##############################################################################################################
# Use the below code for making predictions on new data...
##############################################################################################################

loaded_model_new = load_model('lstm_model_Tomorrow_price_2_step_new.h5')
loaded_scaler = joblib.load('feature_scaler_new1.pkl')
loaded_train_index = np.load('train_indices_new1.npy')

new_data = pd.read_csv('modified_file_with_technical_parameters_new1.csv', na_values=['null'], index_col='Date', parse_dates=True, infer_datetime_format=True)
new_data_transformed = loaded_scaler.transform(new_data[features])
new_data_transformed = pd.DataFrame(columns=features, data=new_data_transformed, index=new_data.index)
new_X = np.array(new_data_transformed).reshape(len(new_data_transformed), 1, len(features))

# Make predictions using the loaded model
new_predictions = loaded_model_new.predict(new_X)

prediction_dates = new_data.index[-len(new_predictions):]
input_features_df = new_data[features + ['Tomorrow Price']].loc[prediction_dates]
predictions_df = pd.DataFrame(data={'Date': prediction_dates, 'Predicted Tomorrow Price': new_predictions.flatten()})

predictions_df['Predicted Tomorrow Price']
output_df = pd.concat([input_features_df, predictions_df], axis=1)

print(output_df)


"""

####################################################################################################################
Methodology - We used past data of ethereum coin to predict initial value of JAN 2nd 2024 - Then we used that value to recalculate values such as
SMAs,EMAs,30-day Avg,7 day Avg etc... We calculated then using data_loader.py and reinserted values to calculate further...
####################################################################################################################

# Import necessary libraries
from enum import Enum

# Define the Ethereum price ranges enum
class ETHPriceRanges(Enum):
    pr_2000_2025 = 1
    pr_2025_2050 = 2
    pr_2050_2075 = 3
    pr_2075_2100 = 4
    pr_2100_2125 = 5
    pr_2125_2150 = 6
    pr_2150_2175 = 7
    pr_2175_2200 = 8
    pr_2200_2225 = 9
    pr_2225_2250 = 10
    pr_2250_2275 = 11
    pr_2275_2300 = 12
    pr_2300_2325 = 13
    pr_2325_2350 = 14
    pr_2350_2375 = 15
    pr_2375_2400 = 16
    pr_2400_2425 = 17
    pr_2425_2450 = 18
    pr_2450_2475 = 19
    pr_2475_2500 = 20
    pr_2500_2525 = 21
    pr_2525_2550 = 22
    pr_2550_2575 = 23
    pr_2575_2600 = 24

def predictions():
    """
    Business logic for predicting Ethereum prices.
    The output is a list of size 7 with values from ETHPriceRanges.
    """
    # Replace this business logic with your own prediction algorithm
    # Here, I'm providing example predictions
    return [
        ETHPriceRanges.pr_2350_2375,
        ETHPriceRanges.pr_2400_2425,
        ETHPriceRanges.pr_2425_2450,
        ETHPriceRanges.pr_2475_2500,
        ETHPriceRanges.pr_2450_2475,
        ETHPriceRanges.pr_2400_2425,
        ETHPriceRanges.pr_2450_2475,
    ]

# Call the predictions function and store the results
preds = predictions()

# Print the final predicted values
print("Final Predictions:")
for val in preds:
    print(val)

