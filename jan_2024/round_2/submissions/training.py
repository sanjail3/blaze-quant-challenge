# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rx2GKb4Sgt9PLza1QUQFgS6i0P13Ovvj
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.impute import SimpleImputer
from keras.models import Sequential, load_model
from keras.layers import Dense, LSTM
from keras.optimizers import Adam
import matplotlib.pyplot as plt

# Load your dataset
data = pd.read_csv('eth_data.csv')

# Choose relevant features for the model
features = ['Adj Close', 'upper_shadow', 'lower_shadow', 'open2close', 'high2low', 'high2mean', 'low2mean',
            'high2median', 'low2median', 'Price', 'Market_cap', 'Supply', 'BlockSize', 'BlockDifficulty',
            'Hash_count', 'Growth Value']

# Select features and target variable
X = data[features].values
y = data['Price'].values

# Use all data points except the last 100 for training
X_train = X[:-100]
y_train = y[:-100]

# Use the last 100 data points for testing
X_test = X[-100:]
y_test = y[-100:]

# Handle missing values (mean imputation)
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Feature scaling
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)

# Build a simple Random Forest Regressor model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

# LSTM model
model = Sequential()
model.add(LSTM(units=50, activation='relu', input_shape=(X_train_scaled.shape[1], 1)))
model.add(Dense(units=1))
model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')

# Reshape the data for LSTM input
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Train the LSTM model
model.fit(X_train_reshaped, y_train, epochs=500, batch_size=32, validation_split=0.2, verbose=2)

# Save the trained LSTM model
model.save("hi.h5")

# Random Forest and LSTM model predictions
rf_predictions = rf_model.predict(X_test_scaled)
lstm_predictions = model.predict(X_test_reshaped).reshape(-1)

# Evaluate the models
mse_rf = mean_squared_error(y_test, rf_predictions)
mae_rf = mean_absolute_error(y_test, rf_predictions)
r2_rf = r2_score(y_test, rf_predictions)

mse_lstm = mean_squared_error(y_test, lstm_predictions)
mae_lstm = mean_absolute_error(y_test, lstm_predictions)
r2_lstm = r2_score(y_test, lstm_predictions)

# Print the results
print("Random Forest Metrics:")
print(f"MSE: {mse_rf}")
print(f"MAE: {mae_rf}")
print(f"R-squared: {r2_rf}")
print("\nLSTM Metrics:")
print(f"MSE: {mse_lstm}")
print(f"MAE: {mae_lstm}")
print(f"R-squared: {r2_lstm}")

# Visualize predictions for the last 100 data points
plt.figure(figsize=(12, 6))
plt.plot(y_test, label='True Prices', color='blue')
plt.plot(rf_predictions, label='Random Forest Predictions', color='green')
plt.plot(lstm_predictions, label='LSTM Predictions', color='orange')
plt.title('Ethereum Price Predictions (Last 100 Data Points)')
plt.xlabel('Data Points')
plt.ylabel('Price')
plt.legend()
plt.show()

